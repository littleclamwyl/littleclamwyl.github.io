<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.2.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"example.com","root":"/","scheme":"Gemini","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}};
  </script>

  <meta name="description" content="要讲武德，要努力学习。">
<meta property="og:type" content="website">
<meta property="og:title" content="浑元形意太极门大弟子">
<meta property="og:url" content="http://example.com/index.html">
<meta property="og:site_name" content="浑元形意太极门大弟子">
<meta property="og:description" content="要讲武德，要努力学习。">
<meta property="og:locale" content="zh_CN">
<meta property="article:author" content="Yulin Wang">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="http://example.com/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : true,
    isPost : false,
    lang   : 'zh-CN'
  };
</script>

  <title>浑元形意太极门大弟子</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">浑元形意太极门大弟子</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
      <p class="site-subtitle" itemprop="description">师承马保国，以武会友。</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a>

  </li>
  </ul>
</nav>




</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content index posts-expand">
            
      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2020/12/02/Python_Learning/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Yulin Wang">
      <meta itemprop="description" content="要讲武德，要努力学习。">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="浑元形意太极门大弟子">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2020/12/02/Python_Learning/" class="post-title-link" itemprop="url">Python Basic Commands</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2020-12-02 10:38:23" itemprop="dateCreated datePublished" datetime="2020-12-02T10:38:23+08:00">2020-12-02</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2020-12-03 15:09:16" itemprop="dateModified" datetime="2020-12-03T15:09:16+08:00">2020-12-03</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="1-Python-基本知识"><a href="#1-Python-基本知识" class="headerlink" title="1. Python 基本知识"></a>1. Python 基本知识</h2><h3 id="一、-优缺点和特点"><a href="#一、-优缺点和特点" class="headerlink" title="一、 优缺点和特点"></a>一、 优缺点和特点</h3><p>代码-&gt;翻译-&gt;机器语言010101</p>
<p>优点：</p>
<p>开源</p>
<p>丰富的第三方库：</p>
<p>web开发-&gt;django/flask/tornado</p>
<p>爬虫-&gt;scrapy</p>
<p>数据分析-&gt;numpy/pandas/scikit-learn</p>
<p>缺点：</p>
<p>GIL全局解释器锁</p>
<p>解释语言（Python）-&gt;解释器-&gt;CPU</p>
<p>编译语言（C）-&gt;CPU-&gt;运行速度更快</p>
<h3 id="二、-注释"><a href="#二、-注释" class="headerlink" title="二、 注释"></a>二、 注释</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#这是单行注释</span></span><br><span class="line"></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27; </span></span><br><span class="line"><span class="string">多行注释</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#特殊注释</span></span><br><span class="line"><span class="comment">#1.针对Linux</span></span><br><span class="line"><span class="comment">#指定Python的解析器路径</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#2.编码注释</span></span><br><span class="line"><span class="comment">#coding = utf-8 </span></span><br></pre></td></tr></table></figure>
<p>ASCII最大字符数256 -&gt; UNICODE 65536个字符，可以包含所有语言的所有字符，就不会乱码了。</p>
<p>UNICODE只规定了每个字符的数字编码，但没有规定这个字符如何存储。</p>
<ol>
<li><p>utf-8/utf-16/utf-32是Unicode的一种具体实现</p>
</li>
<li><p>gbk是中文编码</p>
</li>
</ol>
<p>数据结构 + 算法 = 程序</p>
<p>变量：一段有名字的连续存储空间</p>
<p>可以通过定义变量来申请并命名这样的存储空间。</p>
<p>Python是强类型的数据，不需要定义变量的类型。赋值是什么类型的数据，变量就是什么类型的。</p>
<h3 id="三、-基本操作符"><a href="#三、-基本操作符" class="headerlink" title="三、 基本操作符"></a>三、 基本操作符</h3><p><strong>注意优先级顺序：</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 算术运算符</span></span><br><span class="line">+ <span class="comment">#加</span></span><br><span class="line">- <span class="comment">#减</span></span><br><span class="line">* <span class="comment">#乘</span></span><br><span class="line">/ <span class="comment">#除</span></span><br><span class="line">** <span class="comment">#指数</span></span><br><span class="line">% <span class="comment">#求余数</span></span><br><span class="line">// <span class="comment">#求商去除余数</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 比较运算符</span></span><br><span class="line">== </span><br><span class="line">!=</span><br><span class="line">&gt;</span><br><span class="line">&lt; </span><br><span class="line">&gt;=</span><br><span class="line">&lt;=</span><br><span class="line"></span><br><span class="line"><span class="comment"># 逻辑运算符</span></span><br><span class="line"><span class="keyword">and</span></span><br><span class="line"><span class="keyword">or</span></span><br><span class="line"><span class="keyword">not</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 赋值运算符</span></span><br><span class="line">=</span><br><span class="line">+=</span><br><span class="line">...</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h3 id="四、-格式化输入和输出"><a href="#四、-格式化输入和输出" class="headerlink" title="四、 格式化输入和输出"></a>四、 格式化输入和输出</h3><p><strong>注意</strong>： 前后的个数一致</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 输出</span></span><br><span class="line"><span class="comment"># %占位符</span></span><br><span class="line">name = <span class="string">&#x27;Little&#x27;</span></span><br><span class="line">classPro = <span class="string">&#x27;Class 1&#x27;</span> </span><br><span class="line"></span><br><span class="line"><span class="comment"># 第一种 %d, %s %()</span></span><br><span class="line">print(<span class="string">&quot;我的名字是: %s 来自【%s】&quot;</span> %(name, classPro))</span><br><span class="line">&gt;&gt;我的名字是: Little 来自【Class <span class="number">1</span>】</span><br><span class="line">    </span><br><span class="line"><span class="comment"># 第二种 &#123;&#125; .format()</span></span><br><span class="line">print(<span class="string">&quot;姓名:&#123;&#125;&quot;</span>.<span class="built_in">format</span>(name))</span><br><span class="line">&gt;&gt;姓名:Little</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    </span><br><span class="line"><span class="comment"># 输入    </span></span><br><span class="line"><span class="comment"># input</span></span><br><span class="line"><span class="comment"># 类型都是str</span></span><br><span class="line">name = <span class="built_in">input</span>(<span class="string">&quot;请输入你的姓名：&quot;</span>)</span><br><span class="line">Number = <span class="built_in">input</span>(<span class="string">&quot;请输入你的电话：&quot;</span>)</span><br><span class="line">&gt;&gt;请输入你的姓名：Little</span><br><span class="line">&gt;&gt;请输入你的电话：</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h2 id="2-Python-判断语句和流程控制"><a href="#2-Python-判断语句和流程控制" class="headerlink" title="2. Python 判断语句和流程控制"></a>2. Python 判断语句和流程控制</h2><p>流程：计算机执行代码的顺序</p>
<p>流程控制：对计算机代码执行的顺序进行有效的管理</p>
<p>流程控制的分类：</p>
<ol>
<li><p>顺序 </p>
</li>
<li><p>循环</p>
</li>
<li><p>判断/选择</p>
</li>
</ol>
<h3 id="一、-if-else"><a href="#一、-if-else" class="headerlink" title="一、 if-else"></a>一、 if-else</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 判断/选择</span></span><br><span class="line"><span class="comment">#1. 单分支</span></span><br><span class="line"><span class="keyword">if</span> 条件表达式:</span><br><span class="line">    ...</span><br><span class="line"><span class="comment">#2. 双分支</span></span><br><span class="line"><span class="keyword">if</span> 条件表达式:</span><br><span class="line">    ...</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    ...</span><br><span class="line">    </span><br><span class="line"><span class="comment">#3. 多分支</span></span><br><span class="line"><span class="keyword">if</span> 条件表达式:</span><br><span class="line">    ...</span><br><span class="line">       </span><br><span class="line"><span class="keyword">elif</span> 条件表达式:</span><br><span class="line">    ...</span><br><span class="line">       </span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    ...</span><br><span class="line">    <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#例子1</span></span><br><span class="line">Score = <span class="number">87</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> Score &lt;= <span class="number">60</span>:</span><br><span class="line">    print(<span class="string">&quot;继续努力。&quot;</span>)</span><br><span class="line">    <span class="keyword">pass</span> <span class="comment">#空语句</span></span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    print(<span class="string">&quot;干的不错。&quot;</span>)</span><br><span class="line">print(<span class="string">&quot;语句运行结束。&quot;</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#例子2</span></span><br><span class="line">Score = <span class="built_in">int</span>(<span class="built_in">input</span>(<span class="string">&quot;请输入您的成绩：&quot;</span>))</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> Score &lt;= <span class="number">60</span>:</span><br><span class="line">    print(<span class="string">&quot;继续努力。&quot;</span>)</span><br><span class="line">    <span class="keyword">pass</span> <span class="comment">#空语句</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">elif</span> Score &gt; <span class="number">80</span>:</span><br><span class="line">    print(<span class="string">&quot;做得很好，继续保持。&quot;</span>)</span><br><span class="line">    <span class="keyword">pass</span></span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    print(<span class="string">&quot;干的不错。&quot;</span>)</span><br><span class="line">    <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line">&gt;&gt;请输入您的成绩：<span class="number">55</span></span><br><span class="line">&gt;&gt;继续努力。</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#嵌套</span></span><br><span class="line"><span class="keyword">if</span> 条件<span class="number">1</span>:</span><br><span class="line">    <span class="keyword">if</span> 条件<span class="number">2</span>:</span><br><span class="line">        print(<span class="string">&quot;满足条件1，满足条件2&quot;</span>)</span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        print(<span class="string">&quot;满足条件1，不满足条件2&quot;</span>)</span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line">    <span class="keyword">pass</span></span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    print(<span class="string">&quot;不满足条件1&quot;</span>)</span><br><span class="line">    <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>条件表达式：比较表达式/逻辑表达式/符合表达式</p>
<h3 id="二、-while"><a href="#二、-while" class="headerlink" title="二、 while"></a>二、 while</h3><ol>
<li>有初始值</li>
<li>有条件表达式</li>
<li>自增变量/自减【循环体内计数变量】，否则会死循环</li>
</ol>
<p>特点：循环的次数不确定，是依靠循环条件结束。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">while 条件表达式:</span><br><span class="line">    ...</span><br><span class="line">    </span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">for ... in 集合对象:    </span><br><span class="line">    </span><br><span class="line">  </span><br></pre></td></tr></table></figure>
<h3 id="三、-for"><a href="#三、-for" class="headerlink" title="三、 for"></a>三、 for</h3><h3 id="四、-break-continue"><a href="#四、-break-continue" class="headerlink" title="四、 break, continue"></a>四、 break, continue</h3><h3 id="五、-多条件和短路"><a href="#五、-多条件和短路" class="headerlink" title="五、 多条件和短路"></a>五、 多条件和短路</h3><h2 id="3-Python-数据结构"><a href="#3-Python-数据结构" class="headerlink" title="3. Python 数据结构"></a>3. Python 数据结构</h2><p>Python数据类型：</p>
<p>基础类型：</p>
<ol>
<li>num: int/float/complex/bool</li>
<li>str</li>
</ol>
<p>高级类型（数据结构）：</p>
<ol>
<li>dict 字典</li>
<li>tuple 元组</li>
<li>list 列表</li>
<li>set 集合</li>
</ol>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2020/11/26/mlp-workshop-5/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Yulin Wang">
      <meta itemprop="description" content="要讲武德，要努力学习。">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="浑元形意太极门大弟子">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2020/11/26/mlp-workshop-5/" class="post-title-link" itemprop="url">mlp workshop 5</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              

              <time title="创建时间：2020-11-26 10:31:18 / 修改时间：10:33:32" itemprop="dateCreated datePublished" datetime="2020-11-26T10:31:18+08:00">2020-11-26</time>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="Setup"><a href="#Setup" class="headerlink" title="Setup"></a>Setup</h1><h2 id="1-1-Packages"><a href="#1-1-Packages" class="headerlink" title="1.1 Packages"></a>1.1 Packages</h2><p>First, the version of scikit-learn on noteable is slightly out of date so we will update it if necessary (this may take a minute or two),</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pkg_resources</span><br><span class="line"><span class="keyword">if</span> pkg_resources.get_distribution(<span class="string">&quot;scikit-learn&quot;</span>).version != <span class="string">&#x27;0.22.1&#x27;</span>:</span><br><span class="line">    !conda install --yes scikit-learn</span><br></pre></td></tr></table></figure>
<p>In the cell below we will load the core libraries we will be using for this workshop and setting some sensible defaults for our plot size and resolution. </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Display plots inline</span></span><br><span class="line">%matplotlib inline</span><br><span class="line"></span><br><span class="line"><span class="comment"># Data libraries</span></span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="comment"># Plotting libraries</span></span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> seaborn <span class="keyword">as</span> sns</span><br><span class="line"></span><br><span class="line"><span class="comment"># Plotting defaults</span></span><br><span class="line">plt.rcParams[<span class="string">&#x27;figure.figsize&#x27;</span>] = (<span class="number">8</span>,<span class="number">5</span>)</span><br><span class="line">plt.rcParams[<span class="string">&#x27;figure.dpi&#x27;</span>] = <span class="number">80</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># sklearn modules</span></span><br><span class="line"><span class="keyword">import</span> sklearn</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> mean_squared_error</span><br><span class="line"><span class="keyword">from</span> sklearn.pipeline <span class="keyword">import</span> make_pipeline</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> GridSearchCV, KFold</span><br></pre></td></tr></table></figure>
<hr>
<h2 id="1-2-Helper-Functions"><a href="#1-2-Helper-Functions" class="headerlink" title="1.2 Helper Functions"></a>1.2 Helper Functions</h2><p>Below are two helper functions we will be using throughout the lab.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_coefs</span>(<span class="params">m</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;Returns the model coefficients from a Scikit-learn model object as an array,</span></span><br><span class="line"><span class="string">    includes the intercept if available.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">if</span> m.intercept_ <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">        <span class="keyword">return</span> m.coef_</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> np.concatenate([[m.intercept_], m.coef_])</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">model_fit</span>(<span class="params">m, X, y, plot = <span class="literal">False</span></span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;Returns the root mean squared error of a fitted model based on provided X and y values.</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        m: sklearn model object</span></span><br><span class="line"><span class="string">        X: model matrix to use for prediction</span></span><br><span class="line"><span class="string">        y: outcome vector to use to calculating rmse and residuals</span></span><br><span class="line"><span class="string">        plot: boolean value, should fit plots be shown </span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    </span><br><span class="line">    y_hat = m.predict(X)</span><br><span class="line">    rmse = mean_squared_error(y, y_hat, squared=<span class="literal">False</span>)</span><br><span class="line">    </span><br><span class="line">    res = pd.DataFrame(</span><br><span class="line">        data = &#123;<span class="string">&#x27;y&#x27;</span>: y, <span class="string">&#x27;y_hat&#x27;</span>: y_hat, <span class="string">&#x27;resid&#x27;</span>: y - y_hat&#125;</span><br><span class="line">    )</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">if</span> plot:</span><br><span class="line">        plt.figure(figsize=(<span class="number">12</span>, <span class="number">6</span>))</span><br><span class="line">        </span><br><span class="line">        plt.subplot(<span class="number">121</span>)</span><br><span class="line">        sns.lineplot(x=<span class="string">&#x27;x&#x27;</span>, y=<span class="string">&#x27;y&#x27;</span>, color=<span class="string">&quot;grey&quot;</span>, data =  pd.DataFrame(data=&#123;<span class="string">&#x27;x&#x27;</span>: [<span class="built_in">min</span>(y),<span class="built_in">max</span>(y)], <span class="string">&#x27;y&#x27;</span>: [<span class="built_in">min</span>(y),<span class="built_in">max</span>(y)]&#125;))</span><br><span class="line">        sns.scatterplot(x=<span class="string">&#x27;y&#x27;</span>, y=<span class="string">&#x27;y_hat&#x27;</span>, data=res).set_title(<span class="string">&quot;Fit plot&quot;</span>)</span><br><span class="line">        </span><br><span class="line">        plt.subplot(<span class="number">122</span>)</span><br><span class="line">        sns.scatterplot(x=<span class="string">&#x27;y&#x27;</span>, y=<span class="string">&#x27;resid&#x27;</span>, data=res).set_title(<span class="string">&quot;Residual plot&quot;</span>)</span><br><span class="line">        </span><br><span class="line">        plt.subplots_adjust(left=<span class="number">0.0</span>)</span><br><span class="line">        </span><br><span class="line">        plt.suptitle(<span class="string">&quot;Model rmse = &quot;</span> + <span class="built_in">str</span>(<span class="built_in">round</span>(rmse, <span class="number">4</span>)), fontsize=<span class="number">16</span>)</span><br><span class="line">        plt.show()</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> rmse</span><br></pre></td></tr></table></figure>
<hr>
<h2 id="1-3-Data"><a href="#1-3-Data" class="headerlink" title="1.3 Data"></a>1.3 Data</h2><p>The data for this week’s workshop comes from the Elements of Statistical Learning textbook. The data originally come from a study by Stamey et al. (1989) in which they examined the relationship between the level of prostate-specific antigen (<code>psa</code>) and a number of clinical measures in men who were about to receive a prostatectomy. The variables are as follows,</p>
<ul>
<li><code>lpsa</code> - log of the level of prostate-specific antigen</li>
<li><code>lcavol</code> - log cancer volume</li>
<li><code>lweight</code> - log prostate weight</li>
<li><code>age</code> - patient age</li>
<li><code>lbph</code> - log of the amount of benight prostatic hyperplasia</li>
<li><code>svi</code> - seminal vesicle invasion</li>
<li><code>lcp</code> - log of capsular penetration</li>
<li><code>gleason</code> - Gleason score</li>
<li><code>pgg45</code> - percent of Gleason scores 4 or 5</li>
<li><code>train</code> - test / train split used in ESL</li>
</ul>
<p>These data are available in <code>prostate.csv</code> which is provided with this worksheet. </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">prostate = pd.read_csv(<span class="string">&#x27;prostate.csv&#x27;</span>)</span><br><span class="line">prostate</span><br></pre></td></tr></table></figure>
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }


    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }

</style>

<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>lcavol</th>
      <th>lweight</th>
      <th>age</th>
      <th>lbph</th>
      <th>svi</th>
      <th>lcp</th>
      <th>gleason</th>
      <th>pgg45</th>
      <th>lpsa</th>
      <th>train</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>-0.579818</td>
      <td>2.769459</td>
      <td>50</td>
      <td>-1.386294</td>
      <td>0</td>
      <td>-1.386294</td>
      <td>6</td>
      <td>0</td>
      <td>-0.430783</td>
      <td>T</td>
    </tr>
    <tr>
      <th>1</th>
      <td>-0.994252</td>
      <td>3.319626</td>
      <td>58</td>
      <td>-1.386294</td>
      <td>0</td>
      <td>-1.386294</td>
      <td>6</td>
      <td>0</td>
      <td>-0.162519</td>
      <td>T</td>
    </tr>
    <tr>
      <th>2</th>
      <td>-0.510826</td>
      <td>2.691243</td>
      <td>74</td>
      <td>-1.386294</td>
      <td>0</td>
      <td>-1.386294</td>
      <td>7</td>
      <td>20</td>
      <td>-0.162519</td>
      <td>T</td>
    </tr>
    <tr>
      <th>3</th>
      <td>-1.203973</td>
      <td>3.282789</td>
      <td>58</td>
      <td>-1.386294</td>
      <td>0</td>
      <td>-1.386294</td>
      <td>6</td>
      <td>0</td>
      <td>-0.162519</td>
      <td>T</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0.751416</td>
      <td>3.432373</td>
      <td>62</td>
      <td>-1.386294</td>
      <td>0</td>
      <td>-1.386294</td>
      <td>6</td>
      <td>0</td>
      <td>0.371564</td>
      <td>T</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>92</th>
      <td>2.830268</td>
      <td>3.876396</td>
      <td>68</td>
      <td>-1.386294</td>
      <td>1</td>
      <td>1.321756</td>
      <td>7</td>
      <td>60</td>
      <td>4.385147</td>
      <td>T</td>
    </tr>
    <tr>
      <th>93</th>
      <td>3.821004</td>
      <td>3.896909</td>
      <td>44</td>
      <td>-1.386294</td>
      <td>1</td>
      <td>2.169054</td>
      <td>7</td>
      <td>40</td>
      <td>4.684443</td>
      <td>T</td>
    </tr>
    <tr>
      <th>94</th>
      <td>2.907447</td>
      <td>3.396185</td>
      <td>52</td>
      <td>-1.386294</td>
      <td>1</td>
      <td>2.463853</td>
      <td>7</td>
      <td>10</td>
      <td>5.143124</td>
      <td>F</td>
    </tr>
    <tr>
      <th>95</th>
      <td>2.882564</td>
      <td>3.773910</td>
      <td>68</td>
      <td>1.558145</td>
      <td>1</td>
      <td>1.558145</td>
      <td>7</td>
      <td>80</td>
      <td>5.477509</td>
      <td>T</td>
    </tr>
    <tr>
      <th>96</th>
      <td>3.471966</td>
      <td>3.974998</td>
      <td>68</td>
      <td>0.438255</td>
      <td>1</td>
      <td>2.904165</td>
      <td>7</td>
      <td>20</td>
      <td>5.582932</td>
      <td>F</td>
    </tr>
  </tbody>
</table>
<p>97 rows × 10 columns</p>

</div>



<p>As before we will begin by constructing a pairs plot of our data and examining the relationships between our variables.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sns.pairplot(data=prostate)</span><br></pre></td></tr></table></figure>
<pre><code>&lt;seaborn.axisgrid.PairGrid at 0x7f2b896f8710&gt;
</code></pre><hr>
<h3 id="diams-Exercise-1"><a href="#diams-Exercise-1" class="headerlink" title="&diams; Exercise 1"></a>&diams; Exercise 1</h3><p>Are there any interesting patterns in these data? Specifically, your answer should address,</p>
<ul>
<li>Do any of our variables appear to be categorical / ordinal rather than numeric?</li>
<li><p>Which variable appears likely to have the strongest relationship with <code>lpsa</code>?</p>
</li>
<li><p><em><code>svi</code> and <code>gleason</code> can both potentially be considered categorical. <code>gleason</code> in particular is an ordinal variable</em></p>
</li>
<li><em><code>lcavol</code> seems to have the clearest relationship with <code>lpsa</code>.</em></li>
</ul>
<hr>
<h3 id="diams-Exercise-2"><a href="#diams-Exercise-2" class="headerlink" title="&diams; Exercise 2"></a>&diams; Exercise 2</h3><p>Why do you think we are exploring the relationship between these variables and the log of <code>psa</code> rather than just <code>psa</code>?</p>
<p><em><code>psa</code> is very right skewed, transforming it in this way gives a symmetric and more “normal” distribution.</em></p>
<hr>
<h2 id="1-4-Validation-Set"><a href="#1-4-Validation-Set" class="headerlink" title="1.4 Validation Set"></a>1.4 Validation Set</h2><p>For these data we have already been provided a column to indicate which values should be used for the training set and which for the validation set. This is encoded by the values in the <code>train</code> column - we can use these columns to separate our data and generate our training data: <code>Xt</code> and <code>yt</code> as well as our validation data <code>Xv</code> and <code>yv</code>. As we will also need the complete data set we will also construct <code>X</code> and <code>y</code>, which contain all 97 observations but without the <code>train</code> column.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Create train and validate data frames</span></span><br><span class="line">train = prostate.query(<span class="string">&quot;train == &#x27;T&#x27;&quot;</span>).drop(<span class="string">&#x27;train&#x27;</span>, axis=<span class="number">1</span>)</span><br><span class="line">validate = prostate.query(<span class="string">&quot;train == &#x27;F&#x27;&quot;</span>).drop(<span class="string">&#x27;train&#x27;</span>, axis=<span class="number">1</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Training data</span></span><br><span class="line">Xt = train.drop([<span class="string">&#x27;lpsa&#x27;</span>], axis=<span class="number">1</span>)</span><br><span class="line">yt = train.lpsa</span><br><span class="line"></span><br><span class="line">print(<span class="string">&#x27;Xt:&#x27;</span>, Xt.shape)</span><br><span class="line">print(<span class="string">&#x27;yt:&#x27;</span>, yt.shape)</span><br></pre></td></tr></table></figure>
<pre><code>Xt: (67, 8)
yt: (67,)
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Validation data</span></span><br><span class="line">Xv = validate.drop(<span class="string">&#x27;lpsa&#x27;</span>, axis=<span class="number">1</span>)</span><br><span class="line">yv = validate.lpsa</span><br><span class="line"></span><br><span class="line">print(<span class="string">&quot;Xv:&quot;</span>, Xv.shape)</span><br><span class="line">print(<span class="string">&quot;yv:&quot;</span>, yv.shape)</span><br></pre></td></tr></table></figure>
<pre><code>Xv: (30, 8)
yv: (30,)
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Complete data</span></span><br><span class="line">X = prostate.drop([<span class="string">&#x27;lpsa&#x27;</span>,<span class="string">&#x27;train&#x27;</span>], axis=<span class="number">1</span>)</span><br><span class="line">y = prostate.lpsa</span><br><span class="line"></span><br><span class="line">print(<span class="string">&quot;X:&quot;</span>, X.shape)</span><br><span class="line">print(<span class="string">&quot;y:&quot;</span>, y.shape)</span><br></pre></td></tr></table></figure>
<pre><code>X: (97, 8)
y: (97,)
</code></pre><hr>
<h2 id="1-5-Baseline-model"><a href="#1-5-Baseline-model" class="headerlink" title="1.5 Baseline model"></a>1.5 Baseline model</h2><p>Our first task is to fit a baseline model which we will be able to use as a point of comparison for our subsequent models. A good candidate for this is a simple linear regression model that includes all of our features.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LinearRegression</span><br><span class="line">lm = LinearRegression().fit(Xt, yt)</span><br></pre></td></tr></table></figure>
<p>We can extract the coefficients for the model, which correspond to the variables: <code>intercept</code>, <code>lcavol</code>, <code>lweight</code>, <code>age</code>, <code>lbph</code>, <code>svi</code>, <code>lcp</code>, <code>gleason</code>, and <code>pgg45</code> respectively.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">get_coefs(lm)</span><br></pre></td></tr></table></figure>
<pre><code>array([ 0.42917013,  0.57654319,  0.61402   , -0.01900102,  0.14484808,
        0.73720864, -0.20632423, -0.02950288,  0.00946516])
</code></pre><p>These coefficients have the typical regression interpretation, e.g. for each unit increase in <code>lcavol</code> we expect <code>lpsa</code> to increase by 0.577 on average. These values are not of particular interest for us for this particular problem as we are more interested in the predictive properties of our model(s). To evaluate this we will use the <code>model_fit</code> helper function defined above. </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">model_fit(lm, Xv, yv, plot=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>
<pre><code>0.7219930785731963
</code></pre><p>Primarily we will use this function to obtain the rmse of our model using the validation data (<code>Xv</code> and <code>yv</code>). Note that we fit the model using the training data (<code>Xt</code> and <code>yt</code>). We have also included a fit ($y$ vs $\hat{y}$) and resid ($y$ vs $y-\hat{y}$) plot of these results.</p>
<hr>
<h3 id="diams-Exercise-3"><a href="#diams-Exercise-3" class="headerlink" title="&diams; Exercise 3"></a>&diams; Exercise 3</h3><p>Based on these plots do you see anything in the fit or residual plot that is potentially concerning?</p>
<p><em>Model appears to be shrinking <code>lpsa</code> values towards the overall mean - small values are over predicted while large values are underpredicted.</em></p>
<hr>
<h3 id="diams-Exercise-4"><a href="#diams-Exercise-4" class="headerlink" title="&diams; Exercise 4"></a>&diams; Exercise 4</h3><p>Would you expect the rmse of the model to be better or worse when using the training data (compared to the validation data)? Check your answer using the <code>model_fit</code> function.</p>
<p><em>Most of the time the validation data should have a worse rmse than the training data since fitting the least squares estimate is equivalent to minimizing the rmse</em></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">model_fit(lm, Xt, yt)</span><br></pre></td></tr></table></figure>
<pre><code>0.662721486039448
</code></pre><hr>
<h2 id="1-6-Scaling"><a href="#1-6-Scaling" class="headerlink" title="1.6 Scaling"></a>1.6 Scaling</h2><p>In subsequent sections we will be exploring the use of the Ridge and Lasso regression models which both penalize larger values of $\beta$. While not particularly bad, our baseline model has $\beta$s that ranged from the smallest at 0.0095 to the largest at 0.737 which is about a 78x difference in magnitude. This difference can be made even worse if we were to change the units of one of our features, e.g. changing a measurement in kg to grams would change that coefficient by 1000 which has no effect on the fit of our linear regression model (predictions and other coefficients would be unchanged) but would have a meaningful impact on the estimates given by a Ridge or Lasso regression model, since that coefficient would now dominate the penalty term.</p>
<p>To deal with this issue, the standard approach is to scale <em>all</em> features to be on a common scale before fitting one of these models. The typical scaling approach is to subtract the mean of each feature and then divide by the standard deviation - this results in each feature column having a mean of 0 and a variance of 1. Additionally, the feature values can now be interpreted as the number of standard deviations away from the mean. </p>
<p>Using sklearn we can perform this transformation using the <code>StandardScaler</code> transformer from the <code>preprocessing</code> submodule.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> StandardScaler</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">S = StandardScaler().fit(X)</span><br></pre></td></tr></table></figure>
<p>Note, that when constructing the scaler object we are using the complete data set <code>X</code> and not the training or validation data. Once we have <code>fit</code> the transformer to the complete data we use it to transform the training and validation data separately to generate <code>Xt_scaled</code> and <code>Xv_scaled</code> respectively.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Xt_scaled = S.transform(Xt)</span><br><span class="line">Xv_scaled = S.transform(Xv)</span><br></pre></td></tr></table></figure>
<hr>
<h3 id="diams-Exercise-5"><a href="#diams-Exercise-5" class="headerlink" title="&diams; Exercise 5"></a>&diams; Exercise 5</h3><p>Explain why we are only transforming <code>Xt</code> and <code>Xv</code> and not <code>yt</code> and <code>yv</code>?</p>
<p><em>Only the scaling of the features affects the penalty in Ridge and Lasso - scaling <code>y</code> would change the betas but in a systematic fashion that will be accounted for by a change in $\alpha$, i.e. same predictive performance</em></p>
<hr>
<h3 id="diams-Exercise-6"><a href="#diams-Exercise-6" class="headerlink" title="&diams; Exercise 6"></a>&diams; Exercise 6</h3><p>What are the units of the transformed features in <code>Xt_scaled</code> and <code>Xv_scaled</code>? </p>
<p><em>All of the features are now unitless</em></p>
<hr>
<h2 id="1-7-Scaled-Linear-Regression"><a href="#1-7-Scaled-Linear-Regression" class="headerlink" title="1.7 Scaled Linear Regression"></a>1.7 Scaled Linear Regression</h2><p>Now that we have scaled the data, let us fit another simple linear regression model using these scaled features. </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">lm_scaled = LinearRegression().fit(Xt_scaled, yt)</span><br></pre></td></tr></table></figure>
<p>Once fit we can extract the model coefficients and calculate the validation rmse,</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">get_coefs(lm_scaled)</span><br></pre></td></tr></table></figure>
<pre><code>array([ 2.46493292,  0.67601634,  0.26169361, -0.14073374,  0.20906052,
        0.30362332, -0.28700184, -0.02119493,  0.26557614])
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">model_fit(lm_scaled, Xv_scaled, yv, plot=<span class="literal">False</span>)</span><br></pre></td></tr></table></figure>
<pre><code>0.7219930785731955
</code></pre><hr>
<h3 id="diams-Exercise-7"><a href="#diams-Exercise-7" class="headerlink" title="&diams; Exercise 7"></a>&diams; Exercise 7</h3><p>Using this new model what has changed about our model results? Comment on both the model’s coefficients as well as its predictive performance.</p>
<p><em>Rescaling the features causes the $\beta$s to change but the model is effectively the same and has the same predictive performance as the previous model.</em></p>
<hr>
<h1 id="2-Ridge-Regression"><a href="#2-Ridge-Regression" class="headerlink" title="2. Ridge Regression"></a>2. Ridge Regression</h1><p>Ridge regression is a natural extension to linear regression which introduces an $\ell_2$ penalty on the coefficients to a standard least squares problem. Mathematically, we can express this as the following optimization problem,</p>
<script type="math/tex; mode=display">\underset{\boldsymbol{\beta}}{\text{argmin}} \; \lVert \boldsymbol{y} - \boldsymbol{X} \boldsymbol{\beta} \rVert^2 + \alpha (\boldsymbol{\beta}^T\boldsymbol{\beta})</script><p>The <code>Ridge</code> model is provided by the <code>linear_model</code> submodule and requires a single parameter <code>alpha</code> which determines the tuning parameter for the $\ell_2$ penalty.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> Ridge</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">r = Ridge(alpha=<span class="number">1</span>).fit(Xt_scaled, yt)</span><br><span class="line">model_fit(r, Xv_scaled, yv, plot=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>
<pre><code>0.7160588667932858
</code></pre><hr>
<h3 id="diams-Exercise-8"><a href="#diams-Exercise-8" class="headerlink" title="&diams; Exercise 8"></a>&diams; Exercise 8</h3><p>Adjust the value of <code>alpha</code> in the cell above and rerun it. Qualitatively, how does the model fit change as alpha changes? How does the rmse change?</p>
<p><em>Model fit does not change much but rmse improves as alpha is increased until around $\alpha=20$</em></p>
<hr>
<h3 id="diams-Exercise-9"><a href="#diams-Exercise-9" class="headerlink" title="&diams; Exercise 9"></a>&diams; Exercise 9</h3><p>In Section 1.4 we mentioned the importance of scaling features before fitting a Ridge regression model. The code below fits the Ridge model to the untransformed training data - repeat Exercise 8 using these data. How does the model fit change as alpha changes? How does the rmse change? How does the models performance compare to the previous model?</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">r = Ridge(alpha=<span class="number">1</span>).fit(Xt, yt)</span><br><span class="line">model_fit(r, Xv, yv, plot=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>
<pre><code>0.7220807306906396
</code></pre><p><em>Fit is roughly equivalent and does not change significantly, rmse gets worse as alpha increases, $\alpha=0$ gives the best performance which is worse than performance of the previous model</em></p>
<hr>
<h2 id="2-1-Ridge-beta-s-as-a-function-of-alpha"><a href="#2-1-Ridge-beta-s-as-a-function-of-alpha" class="headerlink" title="2.1 Ridge $\beta$s as a function of $\alpha$"></a>2.1 Ridge $\beta$s as a function of $\alpha$</h2><p>Finally, one of the useful ways of thinking about the behavior of Ridge regression models is to examine the relationship between our choice of $\alpha$ and the resulting $\beta$s relative to the results we would have obtained from the linear regression model. Since Ridge regression is equivalent to linear regression when $\alpha=0$ we can see that as we increase the value of $\alpha$ we are shrinking all of the $\betas$ towards 0 asymptotically.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">alphas = np.logspace(<span class="number">-2</span>, <span class="number">3</span>, num=<span class="number">200</span>) <span class="comment"># from 10^-2 to 10^3</span></span><br><span class="line"></span><br><span class="line">betas = [] <span class="comment"># Store coefficients</span></span><br><span class="line">rmses = [] <span class="comment"># Store validation rmses</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> a <span class="keyword">in</span> alphas:</span><br><span class="line">    m = Ridge(alpha=a).fit(Xt_scaled, yt)</span><br><span class="line">    betas.append(m.coef_) <span class="comment"># We ignore the intercept as it is not included in the l2 penalty and hence not shrunk</span></span><br><span class="line">    rmses.append(model_fit(m, Xv_scaled, yv))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">res = pd.DataFrame(</span><br><span class="line">    data = betas,</span><br><span class="line">    columns = X.columns <span class="comment"># Label columns w/ feature names</span></span><br><span class="line">).assign(</span><br><span class="line">    alpha = alphas,</span><br><span class="line">    rmse = rmses</span><br><span class="line">).melt(</span><br><span class="line">    id_vars = (<span class="string">&#x27;alpha&#x27;</span>, <span class="string">&#x27;rmse&#x27;</span>)</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sns.lineplot(x=<span class="string">&#x27;alpha&#x27;</span>, y=<span class="string">&#x27;value&#x27;</span>, hue=<span class="string">&#x27;variable&#x27;</span>, data=res).set_title(<span class="string">&quot;Coefficients&quot;</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<hr>
<h2 id="2-2-Tuning-with-GridSearchCV"><a href="#2-2-Tuning-with-GridSearchCV" class="headerlink" title="2.2 Tuning with GridSearchCV"></a>2.2 Tuning with GridSearchCV</h2><p>The $\alpha$ in the Ridge regression model is another example of a hyperparameter, and just like the degree in a polynomial model we can use cross validation to attempt to identify the optimal value for our data. As with the polynomial models from last week we will start by using <code>GridSearchCV</code> to employ 5-fold cross validation to determine an optimal $\alpha$.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">alphas = np.linspace(<span class="number">0</span>, <span class="number">15</span>, num=<span class="number">151</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">gs = GridSearchCV(</span><br><span class="line">    Ridge(),</span><br><span class="line">    param_grid=&#123;<span class="string">&#x27;alpha&#x27;</span>: alphas&#125;,</span><br><span class="line">    cv=KFold(<span class="number">5</span>, <span class="literal">True</span>, random_state=<span class="number">1234</span>),</span><br><span class="line">    scoring=<span class="string">&quot;neg_root_mean_squared_error&quot;</span></span><br><span class="line">).fit(Xt_scaled, yt)</span><br></pre></td></tr></table></figure>
<p>Note that we are passing <code>sklearn.model_selection.KFold(5, True, 1234)</code> to the <code>cv</code> argument rather than leaving it to its default. This is because, while not obvious, the prostate data is structured (sorted by <code>lpsa</code> value) and this way we are able to ensure that the folds are properly shuffled. Failing to do this causes <em>very</em> unreliable results from the cross validation process.</p>
<p>Once fit, we can examine the results to determine what value of $\alpha$ was chosen as well as examine the calculated mean of the rmses.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">print(gs.best_params_)</span><br></pre></td></tr></table></figure>
<pre><code>&#123;&#39;alpha&#39;: 6.4&#125;
</code></pre><p>To evaluate this model we can access the <code>best_estimator_</code> model object and use it to obtain an rmse for our validation data.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">model_fit(gs.best_estimator_, Xv_scaled, yv)</span><br></pre></td></tr></table></figure>
<pre><code>0.7006912547188234
</code></pre><hr>
<h3 id="diams-Exercise-10"><a href="#diams-Exercise-10" class="headerlink" title="&diams; Exercise 10"></a>&diams; Exercise 10</h3><p>How does this model compare to the performance of our baseline model? Is it better or worse?</p>
<p><em>This model performs better (smaller rmse)</em></p>
<hr>
<h3 id="diams-Exercise-11"><a href="#diams-Exercise-11" class="headerlink" title="&diams; Exercise 11"></a>&diams; Exercise 11</h3><p>How do the model coefficient for this model compare to the base line model? <em>Hint</em> - be careful about which baseline model you compare with.</p>
<p><em>Coefficients are smaller (closer to 0) - should compare to <code>lm_scaled</code> not <code>lm</code></em></p>
<hr>
<p>To further explore this choice of $\alpha$, we can collect relevant data about the folds and their performance from the <code>cv_results_</code> attribute. In this case we are particularly interested in examining the <code>mean_test_score</code> and the <code>split#_test_score</code> keys since these are used to determine the optimal $\alpha$.</p>
<p>In the code below we extract these data into a data frame by selecting our columns of interest along with the $\alpha$ values used (and transform negative rmse values into positive values).</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">cv_res = pd.DataFrame(</span><br><span class="line">    data = gs.cv_results_</span><br><span class="line">).<span class="built_in">filter</span>(</span><br><span class="line">    <span class="comment"># Extract the split#_test_score, mean_test_score, and std_test_score columns</span></span><br><span class="line">    regex = <span class="string">&#x27;(split[0-9]+|mean)_test_score&#x27;</span></span><br><span class="line">).assign(</span><br><span class="line">    <span class="comment"># Add the alphas as a column</span></span><br><span class="line">    alpha = alphas</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">cv_res.update(</span><br><span class="line">    <span class="comment"># Convert negative rmses to positive</span></span><br><span class="line">    <span class="number">-1</span> * cv_res.<span class="built_in">filter</span>(regex = <span class="string">&#x27;_test_score&#x27;</span>)</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p>This data frame can then be used to plot $\alpha$ against the mean root mean squared value over the 5 folds, to produce the following plot.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sns.lineplot(x=<span class="string">&#x27;alpha&#x27;</span>, y=<span class="string">&#x27;mean_test_score&#x27;</span>, data=cv_res)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p>This plot clearly shows that the value 6.4 is obtained as the minimum of this curve. However, this plot gives us an overly confident view of this choice of this particular value of $\alpha$. Specifically, if instead of just plotting the mean rmse, we also examine the variablility of that estimate as well as examine the $\alpha$ vs rmse curve of each fold we see that these estimates are far noisier than they first appeared and we should take the value $\alpha = 6.4$ with a grain of salt.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">d = cv_res.melt(</span><br><span class="line">    id_vars=(<span class="string">&#x27;alpha&#x27;</span>,<span class="string">&#x27;mean_test_score&#x27;</span>),</span><br><span class="line">    var_name=<span class="string">&#x27;fold&#x27;</span>,</span><br><span class="line">    value_name=<span class="string">&#x27;rmse&#x27;</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">sns.lineplot(x=<span class="string">&#x27;alpha&#x27;</span>, y=<span class="string">&#x27;rmse&#x27;</span>, color=<span class="string">&#x27;black&#x27;</span>, ci = <span class="string">&#x27;sd&#x27;</span>, data = d)  <span class="comment"># Plot the mean rmse +/- the std dev of the rmse.</span></span><br><span class="line">sns.lineplot(x=<span class="string">&#x27;alpha&#x27;</span>, y=<span class="string">&#x27;rmse&#x27;</span>, hue=<span class="string">&#x27;fold&#x27;</span>, data = d, legend = <span class="literal">None</span>) <span class="comment"># Plot the curves for each fold</span></span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p>In the plot above the black line shows the mean rmse across the folds (this is the same curve as shown in the previous plot) and the gray interval indicates + and - 1 standard deviation of the rmses. The other colored cuves shows the rmse curve for each of the different folds.</p>
<hr>
<h3 id="diams-Exercise-12"><a href="#diams-Exercise-12" class="headerlink" title="&diams; Exercise 12"></a>&diams; Exercise 12</h3><p>Why do you think that our cross validation results are so unstable?</p>
<p><em>Most likely cause is due to the relatively small size of the data, with 5 fold cross validation the training size is 53 and test size is 14 observations</em></p>
<hr>
<h2 id="2-3-Tuning-with-RidgeCV"><a href="#2-3-Tuning-with-RidgeCV" class="headerlink" title="2.3 Tuning with RidgeCV"></a>2.3 Tuning with RidgeCV</h2><p>Because the process of identifying the value of $\alpha$ is critical to most uses of Ridge regression, sklearn provides a helpful function called <code>RidgeCV</code> which combines <code>Ridge</code> with <code>GridSearchCV</code>. We import this function from <code>linear_model</code> and fit it in the same way.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> RidgeCV</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">r_cv = RidgeCV(</span><br><span class="line">    alphas = np.linspace(<span class="number">0.1</span>, <span class="number">20</span>, num=<span class="number">200</span>), <span class="comment"># RidgeCV does not allow alpha=0 for some reason</span></span><br><span class="line">    scoring = <span class="string">&quot;neg_root_mean_squared_error&quot;</span></span><br><span class="line">).fit(Xt_scaled, yt)</span><br></pre></td></tr></table></figure>
<p>The resulting model object now has the “optimal” value of $\alpha$ stored in the <code>alpha_</code> attribute which we can access directly.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">r_cv.alpha_</span><br></pre></td></tr></table></figure>
<pre><code>4.1
</code></pre><p>Additionally, the returned object can be used like any other model object to obtain predictions for the fitted model using this value of $\alpha$.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">model_fit(r_cv, Xv_scaled, yv)</span><br></pre></td></tr></table></figure>
<pre><code>0.7048772832703118
</code></pre><hr>
<h3 id="diams-Exercise-12-1"><a href="#diams-Exercise-12-1" class="headerlink" title="&diams; Exercise 12"></a>&diams; Exercise 12</h3><p>This model seems to have arrived at a different optimal value for $\alpha$ compared to using <code>GridSearchCV</code> and it also has a different rmse. Review the documentation for <code>RidgeCV</code>. Can you explain this discrepancy?</p>
<p><em>RidgeCV uses generalized cross validation rather than k-fold cv by default.</em></p>
<hr>
<h3 id="diams-Exercise-13"><a href="#diams-Exercise-13" class="headerlink" title="&diams; Exercise 13"></a>&diams; Exercise 13</h3><p>Refit the model using the RidgeCV in such a way that you obtain a result similar to what was obtained by <code>GridSearchCV</code> (in terms of the optimal $\alpha$ and validation rmse).</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">r_cv2 = RidgeCV(</span><br><span class="line">    alphas=np.linspace(<span class="number">0.1</span>, <span class="number">20</span>, num=<span class="number">200</span>), </span><br><span class="line">    cv=KFold(<span class="number">5</span>, <span class="literal">True</span>, random_state=<span class="number">1234</span>),</span><br><span class="line">    scoring = <span class="string">&quot;neg_root_mean_squared_error&quot;</span></span><br><span class="line">).fit(Xt_scaled, yt)</span><br><span class="line"></span><br><span class="line">print( <span class="string">&quot;alpha:&quot;</span>, r_cv2.alpha_)</span><br><span class="line">print( <span class="string">&quot;rmse:&quot;</span>, model_fit(r_cv2, Xv_scaled, yv) )</span><br></pre></td></tr></table></figure>
<pre><code>alpha: 6.3999999999999995
rmse: 0.7006912547188234
</code></pre><hr>
<h1 id="3-The-Lasso"><a href="#3-The-Lasso" class="headerlink" title="3. The Lasso"></a>3. The Lasso</h1><p>The Lasso is related modeling approach but instead uses an $\ell_1$ penalty on the coefficients. Mathematically, we can express this model as the solution of the following optimization problem,</p>
<script type="math/tex; mode=display">\underset{\boldsymbol{\beta}}{\text{argmin}} \; \lVert \boldsymbol{y} - \boldsymbol{X} \boldsymbol{\beta} \rVert_2^2 + \alpha \lVert \boldsymbol{\beta} \rVert_1</script><p>As with the other models from this worksheet, the <code>Lasso</code> model is provided by the <code>linear_model</code> submodule and requires the choice of penalty parameter <code>alpha</code> to determine the importance of the $\ell_1$ penalty.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> Lasso</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">l = Lasso(alpha=<span class="number">0.15</span>).fit(Xt_scaled, yt)</span><br><span class="line">print(<span class="string">&quot;lasso rmse:&quot;</span>, model_fit(l, Xv_scaled, yv, plot=<span class="literal">True</span>))</span><br><span class="line">print(<span class="string">&quot;lasso coefs:&quot;</span>, get_coefs(l))</span><br></pre></td></tr></table></figure>
<pre><code>lasso rmse: 0.6758018477937713
lasso coefs: [ 2.46667716  0.54132589  0.20509456 -0.          0.04944616  0.1265567
  0.          0.          0.03884322]
</code></pre><hr>
<h3 id="diams-Exercise-14"><a href="#diams-Exercise-14" class="headerlink" title="&diams; Exercise 14"></a>&diams; Exercise 14</h3><p>Adjust the value of <code>alpha</code> in the cell above and rerun it. How does the model fit change as alpha changes? How does the validation rmse change?</p>
<p><em>Large values of $\alpha$ negatively impact the model fit as the $\beta$s are shrunk to 0, values around 0.1 to 0.2 produce an improved rmse</em></p>
<hr>
<h2 id="3-1-Lasso-beta-s-as-a-function-of-alpha"><a href="#3-1-Lasso-beta-s-as-a-function-of-alpha" class="headerlink" title="3.1 Lasso $\beta$s as a function of $\alpha$"></a>3.1 Lasso $\beta$s as a function of $\alpha$</h2><p>As with Ridge regression we can examine the values of $\beta$ we obtain as tuning parameter $\alpha$ is adjusted. </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">alphas = np.linspace(<span class="number">0.01</span>, <span class="number">1</span>, num=<span class="number">100</span>)</span><br><span class="line">betas = [] <span class="comment"># Store coefficients</span></span><br><span class="line">rmses = [] <span class="comment"># Store validation rmses</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> a <span class="keyword">in</span> alphas:</span><br><span class="line">    m = Lasso(alpha=a).fit(Xt_scaled, yt)</span><br><span class="line">    betas.append(m.coef_) <span class="comment"># Again ignore the intercept since it isn&#x27;t included in the penalty</span></span><br><span class="line">    rmses.append(model_fit(m, Xv_scaled, yv))</span><br><span class="line"></span><br><span class="line">res = pd.DataFrame(</span><br><span class="line">    data = betas,       <span class="comment"># Coefficients</span></span><br><span class="line">    columns = X.columns <span class="comment"># Coefficient names</span></span><br><span class="line">).assign(</span><br><span class="line">    alpha = alphas,     <span class="comment"># Add alpahs</span></span><br><span class="line">    rmse = rmses        <span class="comment"># Add validation rmses</span></span><br><span class="line">).melt(</span><br><span class="line">    id_vars = (<span class="string">&#x27;alpha&#x27;</span>, <span class="string">&#x27;rmse&#x27;</span>) <span class="comment"># Move columns into the rows</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sns.lineplot(x=<span class="string">&#x27;alpha&#x27;</span>, y=<span class="string">&#x27;value&#x27;</span>, hue=<span class="string">&#x27;variable&#x27;</span>, data=res).set_title(<span class="string">&quot;Coefficients&quot;</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<hr>
<h3 id="diams-Exercise-14-1"><a href="#diams-Exercise-14-1" class="headerlink" title="&diams; Exercise 14"></a>&diams; Exercise 14</h3><p>How does the relationship between the $\beta$s and $\alpha$ differ from what we saw with the Ridge regression results. </p>
<p><em>Values of $\alpha$ are much smaller and we see more rapid shrinkage towards 0 and then coefs exactly equaling 0.</em></p>
<hr>
<h2 id="3-2-Tuning-Lasso"><a href="#3-2-Tuning-Lasso" class="headerlink" title="3.2 Tuning Lasso"></a>3.2 Tuning Lasso</h2><p>We can again use the <code>GridSearchCV</code> function to tune our Lasso model and optimize the $\alpha$ hyperparameter. We avoid using $\alpha = 0$ as this causes a warning due to the fitting method (coordinate descent) not converging well without regularization (the $\ell_1$ penalty here).</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">alphas = np.linspace(<span class="number">0.01</span>, <span class="number">1</span>, num=<span class="number">100</span>)</span><br><span class="line"></span><br><span class="line">l_gs = GridSearchCV(</span><br><span class="line">    Lasso(),</span><br><span class="line">    param_grid=&#123;<span class="string">&#x27;alpha&#x27;</span>: alphas&#125;,</span><br><span class="line">    cv=KFold(<span class="number">5</span>, <span class="literal">True</span>, random_state=<span class="number">1234</span>),</span><br><span class="line">    scoring=<span class="string">&quot;neg_root_mean_squared_error&quot;</span></span><br><span class="line">).fit(Xt_scaled, yt)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">print( <span class="string">&quot;best alpha:&quot;</span>, l_gs.best_params_[<span class="string">&#x27;alpha&#x27;</span>])</span><br><span class="line">print( <span class="string">&quot;best rmse :&quot;</span>, l_gs.best_score_ * <span class="number">-1</span>)</span><br><span class="line">print( <span class="string">&quot;validation rmse:&quot;</span>, model_fit(l_gs.best_estimator_, Xv_scaled, yv) )</span><br></pre></td></tr></table></figure>
<pre><code>best alpha: 0.01
best rmse : 0.8421514849503193
validation rmse: 0.7063579713283387
</code></pre><p>Worryingly, the chosen alpha is the smallest value provided to our grid search, and hence it has selected the model closest to a linear regression model. We can investigate this further by plotting $\alpha$ versus the <code>mean_test_score</code> values from the <code>cv_results_</code> attribute. </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">l_cv_res = pd.DataFrame().assign(</span><br><span class="line">    alpha = alphas,</span><br><span class="line">    rmse = <span class="number">-1</span> * l_gs.cv_results_[<span class="string">&#x27;mean_test_score&#x27;</span>],           <span class="comment"># mean of the rmse over the folds</span></span><br><span class="line">    rmse_se = l_gs.cv_results_[<span class="string">&#x27;std_test_score&#x27;</span>] / np.sqrt(l_gs.n_splits_), <span class="comment"># standard error of the rmse</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">plt.figure(figsize=(<span class="number">14</span>, <span class="number">6</span>))</span><br><span class="line">        </span><br><span class="line">plt.subplot(<span class="number">121</span>)</span><br><span class="line">sns.lineplot(x=<span class="string">&#x27;alpha&#x27;</span>, y=<span class="string">&#x27;rmse&#x27;</span>, data=l_cv_res)</span><br><span class="line"></span><br><span class="line">plt.subplot(<span class="number">122</span>)</span><br><span class="line">sns.lineplot(x=<span class="string">&#x27;alpha&#x27;</span>, y=<span class="string">&#x27;rmse&#x27;</span>, data=l_cv_res).set_xscale(<span class="string">&#x27;log&#x27;</span>)</span><br></pre></td></tr></table></figure>
<p>In this case it appears that the model’s rmses nearly monotonically increase as $\alpha$ increases. This indicates that the CV proceedure is exhibiting a preference for the linear regression mode, i.e. a lasso model with no shrinkage. We can check this explicitly by fitting the <code>LinearRegression</code> with <code>GridSearchCV</code> and comparing the cross validation rmse, this is necessary because our previous modeling did not use any CV to calculate the rmse.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">GridSearchCV(</span><br><span class="line">    LinearRegression(),</span><br><span class="line">    param_grid = &#123;&#125;,</span><br><span class="line">    cv=KFold(<span class="number">5</span>, <span class="literal">True</span>, random_state=<span class="number">1234</span>),</span><br><span class="line">    scoring=<span class="string">&quot;neg_root_mean_squared_error&quot;</span></span><br><span class="line">).fit(</span><br><span class="line">    Xt_scaled, yt</span><br><span class="line">).best_score_ * <span class="number">-1</span></span><br></pre></td></tr></table></figure>
<pre><code>0.8379041282149198
</code></pre><p>In this case the linear regression model does produce a smaller mean rmse than any of the Lasso models. This suggests our choice of model should just be the original linear regression model.</p>
<p><em>Aside</em> - However, if we examine these plots closesly, values of $\alpha$ between 0.01 and 0.1 have very similar mean rmses and there is inherent uncertainty in our estimates of these rmses based on the cross validation folds and the size of our data. One of the suggestions employed by Hastie, et al. in their <code>glmnet</code> R package is instead of using the $\alpha$ with the smallest mean rmse to instead use the largest value of $\alpha$ that has an error metric (rmse) that is within 1 standard <em>error</em> of the minimum value of the error metric (rmse). We can find this value using the <code>l_cv_res</code> data frame we previously constructed.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">i = l_cv_res.rmse.idxmin()</span><br><span class="line"></span><br><span class="line">min_rmse = l_cv_res.rmse[i]       <span class="comment"># Smallest rmse</span></span><br><span class="line">min_rmse_se = l_cv_res.rmse_se[i] <span class="comment"># Std error of the smallest rmse</span></span><br><span class="line"></span><br><span class="line">sub = l_cv_res.rmse &lt;= min_rmse + min_rmse_se <span class="comment"># Find rmses w/in 1se of the min + se</span></span><br><span class="line"></span><br><span class="line">alpha_1se = l_cv_res.alpha[ sub ].<span class="built_in">max</span>() <span class="comment"># Find the largest alpha</span></span><br><span class="line"></span><br><span class="line">alpha_1se</span><br></pre></td></tr></table></figure>
<pre><code>0.27
</code></pre><p>While this approach seems plausible / practical, it should be treated as at best a heuristic as I have not been able to track down any theoretical support for it. Note that we could also employ this strategy even if the minimal $\alpha$ had not been approximately 0 and it is still likely to be helpful as any increase in $\alpha$ is likely to reduce the number of coefficients in the final model. </p>
<hr>
<h3 id="diams-Exercise-15"><a href="#diams-Exercise-15" class="headerlink" title="&diams; Exercise 15"></a>&diams; Exercise 15</h3><p>If you were to use the $\alpha_\text{1se} = 0.27$ which variables would be excluded from the model? </p>
<p><em>‘age’, ‘lbph’, ‘lcp’, ‘gleason’, ‘pgg45’ would all have coefficients of 0 and hence be excluded from the model.</em></p>
<hr>
<h3 id="diams-Exercise-16"><a href="#diams-Exercise-16" class="headerlink" title="&diams; Exercise 16"></a>&diams; Exercise 16</h3><p>If you were to use the $\alpha_{\text{1se}} = 0.27$ what would the validation rmse be for this model? How does this compare to the other models we’ve examined so far? Why might we still prefer this model over the linear regression or Ridge model?</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">m = Lasso(alpha=<span class="number">0.27</span>).fit(Xt_scaled, yt)</span><br><span class="line">model_fit(m, Xv_scaled, yv)</span><br></pre></td></tr></table></figure>
<pre><code>0.7185033561718017
</code></pre><p><em>This model has slightly better performance than the Linear Regression model (0.722) but worse than the Ridge model (0.700), however we might prefer it as it is a far simpler model with fewer coefficients (only 4 including the intercept) than either of the alternatives.</em></p>
<hr>
<h2 id="4-Concluding-Remarks"><a href="#4-Concluding-Remarks" class="headerlink" title="4 Concluding Remarks"></a>4 Concluding Remarks</h2><p>It is important to notice that throughout the previous two sections we have taken great pains to avoid using our validation data to in any way inform our choice of the tuning parameter. Instead, we have always used <code>KFold</code> with our training data to obtain the necessary metrics for optimizing the $\alpha$ hyperparameter. This would also have been possible using the complete data <code>X</code> and would have slightly improved our rmse estimates due to the slightly larger sample sizes in the test train splits but it would then mean were repeated using the validation data in the process of determining $\alpha$ which then puts us at risk for overfitting and therefore having an overly optimistic view of our model’s uncertainty.  </p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2020/11/25/Machine-Learning-Data-Structure-1st/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Yulin Wang">
      <meta itemprop="description" content="要讲武德，要努力学习。">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="浑元形意太极门大弟子">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2020/11/25/Machine-Learning-Data-Structure-1st/" class="post-title-link" itemprop="url">Machine Learning Data Structure</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              

              <time title="创建时间：2020-11-25 11:10:04 / 修改时间：14:26:55" itemprop="dateCreated datePublished" datetime="2020-11-25T11:10:04+08:00">2020-11-25</time>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="一、机器学习"><a href="#一、机器学习" class="headerlink" title="一、机器学习"></a>一、机器学习</h1><p>中心极限定理CLT</p>
<p>随机变量序列部分和渐进于正态分布</p>
<h2 id="1-Train-Validation-Test"><a href="#1-Train-Validation-Test" class="headerlink" title="1. Train/Validation/Test"></a>1. Train/Validation/Test</h2><p>一个数据集合，可以划分为train data set, validation data set, test data set.</p>
<p>1）其中train用来训练模型，比如你可以得到由10个方法SVM, linear regression, regression tree等等所对应的10个模型，并且这时候你已经对参数进行了调整，以求达到一个非常好的效果。每个model你有各自对应的10组Parameters，那么就有100个训练出来的模型。</p>
<p>2）此时你要评估一下各自的效果，防止over fit，当然，under fit也是可以防止的，但是模型选择的时候，大家都是往精度高了的模型选择，极少出现选择under fit的模型。这个时候，帮你从100个model中选出来你认为表现最好的1个model，当然实际上你可以选择几个，当作待选的模型。并且通过train error和validation error的对比，大概可以找到那个u-shape的最低点。选择那个时候的model。</p>
<p>3）test只用来测试模型，来看这个模型究竟有多好，就是评价这个模型的泛化能力（generalization）。 这时候，这个model在test上得到的accuracy就是一个很有代表性（representative）的accuracy，以后再在新的数据集上测试时，一般来说是跑不离这个精度的范围的。</p>
<p>此时你通过1和2的反复使用，最终选择了几个模型，可能是完全不同的Linear，SVM，也可能是一个Linear对应的两组模型。总之，你得到了最终的几个模型，并且给出了他们的泛化能力，最终选择哪个，看决策者了。</p>
<h2 id="2-Introduction"><a href="#2-Introduction" class="headerlink" title="2. Introduction"></a>2. Introduction</h2><p>机器学习的分类：监督学习和非监督学习</p>
<p>监督学习又分为：==regression==和==classification==。当然也可以分为prediction和inference两种。前者注重依靠现有数据，对将来可能发生的情况进行预测；后者注重研究哪些factor对response variable有重大影响。</p>
<p>非监督学习：discovering clusters和discovering latent factors</p>
<ol>
<li>linear regression: if data is from normal distribution, since y = $w^{T}x$ + $\epsilon$ is from $N(w^{T}x, \sigma^{2})$, then OLS = MLE. </li>
</ol>
<ol>
<li>nonlinear regression:多项式/regression tree/kernel machines(radial basis function/logistic sigmoid)</li>
</ol>
<p>Note: pandas.dataframe是二维的，pandas.series是一维的</p>
<p>由给定样本集合求解随机变量的分布密度函数问题是概率统计学的基本问题之一。解决这一问题的方法包括参数估计和非参数估计。</p>
<p>如果反过来，就需要使用Metropolis-Hastings and Gibbs Sampling</p>
<p>参数估计：Maximum Likelihood Estimation(MLE)</p>
<p>非参数估计：Kernel density estimation(KDE)，由数据到分布。不平滑内核，平滑内核。</p>
<p>一般使用高斯内核，是平滑的。</p>
<p>平均积分平方误差的大小来确定带宽h。</p>
<p>pdf比较平坦，则选择较大的h，反之就选择较小的h。</p>
<h2 id="4-Classification"><a href="#4-Classification" class="headerlink" title="4. Classification"></a>4. Classification</h2><p>the type of the response variable are qualitative/categorical</p>
<p>the method is called classification</p>
<p>通常first predict the probability of each of the categories of a qualitative variable</p>
<ol>
<li>Logistic Regression</li>
<li>Linear discriminant analysis</li>
<li>K-nearest neightbours</li>
</ol>
<p>The following are computer-intensive</p>
<ol>
<li>Generalized Additive Models</li>
<li>Trees</li>
<li>SVM</li>
</ol>
<p>问题：依据predictor/feature，判断储户是否会default<code>逾期</code>不还欠款。</p>
<p>X1: balance</p>
<p>X2: income</p>
<p>==Note:== If taking some diseases as examples,  an-made ordinal variable implies an ordering on the outcomes or predictors, </p>
<p>and insist on the difference between some of them are equal and some are not 实际上这是不对的，人为构造顺序关系更容易引入误差</p>
<p>$Pr(default = Yes|balance)$</p>
<p>In fact, there is another variable income, but it seems less relative</p>
<p>the probability for default = Yes(y = 1), if the bank is conservative, the threshold could be lower, such as threshold = 0.1</p>
<script type="math/tex; mode=display">
p(X) = \frac{e^{g(X)}}{1+e^{g(X)}}</script><p>Method: MLE</p>
<p>log-odds/logit</p>
<p>In fact, every training has an estimated coef, Std.error, Z-statistic and P-value</p>
<p>A low P-value means we should include the predictor</p>
<script type="math/tex; mode=display">
log(\frac{p(X)}{1-p(X)})=\beta_{0}+\beta_{1}X_{1}+...+\beta_{p}X_{p}</script><p>其中具有2个离散值的predictors，是可以使用dummy variable的</p>
<p>==Note==那么具有多个离散值的predictors呢？</p>
<p>很经典的问题，如果考虑是否具有student这个身份。同等balance情况下，student不会default。但是总体来说student和balance正相关，student往往具有更高的balance因而更容易default</p>
<p>2 Response Classes</p>
<h3 id="4-2-LDA"><a href="#4-2-LDA" class="headerlink" title="4.2 LDA"></a>4.2 LDA</h3><ol>
<li>当数据实际well-separated时，LR不稳定</li>
<li>样本量小的时候，X is approximately normal in each of the classes, LDA更稳定</li>
<li>在More classes表现更好</li>
</ol>
<script type="math/tex; mode=display">
f_{k}(X) = Pr(X=x|Y=k)\\
Pr(Y=k|X=x) = \frac{\pi_{k}f_{k}(x)}{\sum_{l=1}^{K}\pi_{l}f_{l}(x)}</script><p>$\pi_{k}$=k类样本数量/总样本数</p>
<h1 id="二、-算法和他们的目的"><a href="#二、-算法和他们的目的" class="headerlink" title="二、 算法和他们的目的"></a>二、 算法和他们的目的</h1><h3 id="1-EM-Expectation-and-Maximum"><a href="#1-EM-Expectation-and-Maximum" class="headerlink" title="1. EM(Expectation and Maximum)"></a>1. EM(Expectation and Maximum)</h3><p>估计参数值</p>
<ol>
<li>求期望</li>
<li>期望最大</li>
<li>重复上述两步直至收敛</li>
</ol>
<h3 id="2-MLE-Maximum-Likelihood-Estimate"><a href="#2-MLE-Maximum-Likelihood-Estimate" class="headerlink" title="2. MLE(Maximum Likelihood Estimate)"></a>2. MLE(Maximum Likelihood Estimate)</h3><p>估计参数值</p>
<ol>
<li>写出似然函数</li>
<li>求出使似然函数最大的参数取值</li>
</ol>
<h3 id="3-HMM-Hidden-Markov-Model"><a href="#3-HMM-Hidden-Markov-Model" class="headerlink" title="3. HMM(Hidden Markov Model)"></a>3. HMM(Hidden Markov Model)</h3><p>求概率</p>
<ol>
<li>状态序列</li>
<li>观测序列</li>
</ol>
<h3 id="4-MICE-Multiple-Imputation-and-Chained-Equations"><a href="#4-MICE-Multiple-Imputation-and-Chained-Equations" class="headerlink" title="4. MICE(Multiple Imputation and Chained Equations)"></a>4. MICE(Multiple Imputation and Chained Equations)</h3><p>填补缺失值</p>
<ol>
<li>impute</li>
<li>analyse</li>
<li>pool</li>
</ol>
<h3 id="5-Metropolis-Hastings-amp-Gibbs-Sampling"><a href="#5-Metropolis-Hastings-amp-Gibbs-Sampling" class="headerlink" title="5. Metropolis Hastings &amp; Gibbs Sampling"></a>5. Metropolis Hastings &amp; Gibbs Sampling</h3><p>已知函数分布，从中进行采样。</p>
<h3 id="6-KDE-Kernel-Density-Estimation"><a href="#6-KDE-Kernel-Density-Estimation" class="headerlink" title="6. KDE(Kernel Density Estimation)"></a>6. KDE(Kernel Density Estimation)</h3><p>估计样本分布</p>
<h3 id="7-退火SA-爬山CH"><a href="#7-退火SA-爬山CH" class="headerlink" title="7. 退火SA/爬山CH"></a>7. 退火SA/爬山CH</h3><h3 id="8-梯度下降且采用一阶导数-Gradient-Descent-GD采用二阶导数-Newton-Raphson"><a href="#8-梯度下降且采用一阶导数-Gradient-Descent-GD采用二阶导数-Newton-Raphson" class="headerlink" title="8. 梯度下降且采用一阶导数 Gradient Descent/GD采用二阶导数 Newton-Raphson"></a>8. 梯度下降且采用一阶导数 Gradient Descent/GD采用二阶导数 Newton-Raphson</h3><h1 id="三、数据结构"><a href="#三、数据结构" class="headerlink" title="三、数据结构"></a>三、数据结构</h1><h2 id="1-绪论：数据的概念-逻辑结构与物理结构"><a href="#1-绪论：数据的概念-逻辑结构与物理结构" class="headerlink" title="1. 绪论：数据的概念/逻辑结构与物理结构"></a>1. 绪论：数据的概念/逻辑结构与物理结构</h2><ol>
<li><p>逻辑结构：集合结构/线性结构/树形结构/图形结构</p>
</li>
<li><p>物理结构：顺序存储结构 vs. 链式存储结构</p>
</li>
</ol>
<h2 id="2-算法-时间复杂度与空间复杂度"><a href="#2-算法-时间复杂度与空间复杂度" class="headerlink" title="2. 算法/时间复杂度与空间复杂度"></a>2. 算法/时间复杂度与空间复杂度</h2><p>算法：解决特定问题求解步骤的描述</p>
<p>算法的特性：输入，输出，有穷性，确定性，可行性</p>
<p>时间复杂度—比较容易理解:</p>
<ol>
<li><p>$O(1)$ constant​</p>
</li>
<li><p>$O(n)$ power </p>
</li>
<li><p>$O(log_{10}n)$ logarithm</p>
</li>
</ol>
<p>线性表，顺序存储结构。</p>
<p>链式存储结构。 数据域，指针域。相比较于线性表，减少了增删的时间复杂度，增加了查询的时间复杂度。</p>
<h2 id="3-线性表：顺序存储-链式存储：单链表（着重）、循环链表、静态链表、双向链表"><a href="#3-线性表：顺序存储-链式存储：单链表（着重）、循环链表、静态链表、双向链表" class="headerlink" title="3. 线性表：顺序存储/链式存储：单链表（着重）、循环链表、静态链表、双向链表"></a>3. 线性表：顺序存储/链式存储：单链表（着重）、循环链表、静态链表、双向链表</h2><h2 id="4-栈（顺序与链式）：先进后出-队列（顺序与循环）：先进先出"><a href="#4-栈（顺序与链式）：先进后出-队列（顺序与循环）：先进先出" class="headerlink" title="4. 栈（顺序与链式）：先进后出/队列（顺序与循环）：先进先出"></a>4. 栈（顺序与链式）：先进后出/队列（顺序与循环）：先进先出</h2><h2 id="5-串-简单的模式匹配-KMP算法"><a href="#5-串-简单的模式匹配-KMP算法" class="headerlink" title="5. 串/简单的模式匹配/KMP算法"></a>5. 串/简单的模式匹配/KMP算法</h2><h2 id="6-树"><a href="#6-树" class="headerlink" title="6. 树"></a>6. 树</h2><p>一些概念：</p>
<p>度degree/深度depth/有序性和无序性</p>
<p>存储结构用顺序存储比较困难。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">graph TD</span><br><span class="line"></span><br><span class="line">树的存储结构 --&gt; 双亲表示法</span><br><span class="line">树的存储结构 --&gt; 孩子表示法</span><br><span class="line">树的存储结构 --&gt; 兄弟表示法</span><br><span class="line">兄弟表示法 --&gt; 二叉树的概念</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>二叉树是有序的。</p>
<p>二叉树的存储结构。</p>
<p>因为二叉树比较特殊，所以使用顺序存储也有了可能。</p>
<p>遍历二叉树。</p>
<p>前序遍历/中序遍历/后序遍历/层序遍历</p>
<p>前三种遍历方式，本质上也是一种递归。</p>
<p>print/访问左节点/访问右节点。这是前序遍历</p>
<p>访问左节点，print，访问右节点，这是中序。</p>
<p>访问左节点，访问右节点，print，这是后序遍历。</p>
<p>那么二叉树的建立，就可以参照他的遍历方式。只不过把print变成赋值。</p>
<p>树，森林，二叉树的转换。</p>
<p>赫夫曼树：带权路径长度WPL最小的二叉树。 weighted path length?</p>
<p>树的路径长度，权重</p>
<p>赫夫曼编码</p>
<h2 id="7-图"><a href="#7-图" class="headerlink" title="7. 图"></a>7. 图</h2><h2 id="9-排序算法"><a href="#9-排序算法" class="headerlink" title="9. 排序算法"></a>9. 排序算法</h2><ol>
<li>冒泡排序</li>
<li>简单选择排序</li>
<li>直接插入排序</li>
</ol>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2020/11/25/Exploratory-Data-Analysis-1st/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Yulin Wang">
      <meta itemprop="description" content="要讲武德，要努力学习。">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="浑元形意太极门大弟子">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2020/11/25/Exploratory-Data-Analysis-1st/" class="post-title-link" itemprop="url">Exploratory Data Analysis</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              

              <time title="创建时间：2020-11-25 11:09:52 / 修改时间：14:24:28" itemprop="dateCreated datePublished" datetime="2020-11-25T11:09:52+08:00">2020-11-25</time>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="Explanatory-Data-Analysis"><a href="#Explanatory-Data-Analysis" class="headerlink" title="Explanatory Data Analysis"></a>Explanatory Data Analysis</h1><h2 id="0-引言"><a href="#0-引言" class="headerlink" title="0. 引言"></a>0. 引言</h2><p><strong>1.</strong> 探索性数据分析/机器学习/深度学习等等，一般用到数据，并且分析数据，以期获得inference或者predict的，都可以借用这一套流程。</p>
<p><strong>2.</strong> 很多时候需要构建出一个目标函数，以期获得最优解/满意解。</p>
<p><strong>3. Feature Selection</strong> 决定机器学习上限</p>
<p><strong>4. Model Selection</strong> 提高机器学习下限</p>
<p><strong>5. 分析一下属于Regression还是Classification</strong></p>
<h2 id="1-导入包"><a href="#1-导入包" class="headerlink" title="1. 导入包"></a>1. 导入包</h2><p>==Note== 在下载Python Extension Packages的扩展API时/在使用github时/MySQL……安装，运行时总会遇到千奇百怪的问题。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># R可视化效果也不错，ggplot2 and shiny</span></span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line">print(os.path.abspath(<span class="string">&#x27;.&#x27;</span>)) <span class="comment">#先看看工作路径</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 探索性学习老五样</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> matplotlib <span class="keyword">as</span> mpl</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> seaborn <span class="keyword">as</span> sns</span><br><span class="line">sns, set_style(<span class="string">&quot;whitegrid&quot;</span>)</span><br><span class="line">%matplotlib inline <span class="comment">#jupyter中能够将可视化的图片在结果中展示出来</span></span><br><span class="line"><span class="keyword">import</span> warnings</span><br><span class="line">warnings.filterwarings(<span class="string">&quot;ignore&quot;</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>对数据的初始处理。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">data = pd.read_csv(<span class="string">&quot;Name.csv&quot;</span>)</span><br><span class="line"></span><br><span class="line">data.info() <span class="comment">#看看是否有缺失数据，看看数据的各种类型。可以参考R中的MICE包，比较常见。</span></span><br><span class="line">data.column.fillna(<span class="string">&#x27;target value&#x27;</span>, inplace = <span class="literal">True</span>) <span class="comment">#True将原始数据也修改了，False仅仅修改视图</span></span><br><span class="line"></span><br><span class="line">data.head() <span class="comment">#看看数据构成，具体有哪些features</span></span><br><span class="line"></span><br><span class="line">pd.get_dummies(data.column) <span class="comment">#将目标categorical column变成dummy variable</span></span><br><span class="line"></span><br><span class="line">data.describe() <span class="comment">#查看一下数值型数据mean, standard deviation,  standard error, min, max, 25/50/75 quantile，可以发现一些异常值</span></span><br><span class="line"></span><br><span class="line">data.column.value_counts() <span class="comment">#重复行计数</span></span><br><span class="line"></span><br><span class="line">data.column.unique() <span class="comment">#对于离散型的feature, 看看有多少种类</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<ol>
<li>进行数据切分，可以根据某一个special column，处理tidy data</li>
<li>合并或者转化成New column</li>
<li>Principle Component Analysis进行dimension reduction</li>
<li>Normalize continuous features and neutralize skewness，提高数据集中程度，right-skewed进行log，left-skew进行power。都是为了方便regression/classification</li>
<li>总之，依据上述一些步骤，结合某些最基础的机器学习方法，例如linear regression和logistic regression, 得到一个baseline model.</li>
</ol>
<h2 id="2-特征清洗"><a href="#2-特征清洗" class="headerlink" title="2. 特征清洗"></a>2. 特征清洗</h2><h3 id="1-Missing-Data-Process"><a href="#1-Missing-Data-Process" class="headerlink" title="1. Missing Data Process"></a>1. Missing Data Process</h3><p>查看一下数据的缺失pattern和缺失mechanism。</p>
<p>Pattern可能是单一，连续的，随机的。</p>
<p>Mechanism可能是MCAR, MNAR, MAR。</p>
<p>除了基于经验判断Mechanisim之外，也可以依靠画scatterplot来区分。</p>
<p>==Note== 处理缺失值有许多的方法。像KNN，Random forest本身也是一种预测：依靠 $y_{obs}$ 去预测 $y_{mis}$的值。可以groupby某些特定列，来看缺失值是否是MAR.</p>
<p>常用mean imputation, 该方法会降低variance, 但是比较快速。</p>
<p>multiple imputation更完善，填补多列有缺失的情况，体现了一个递归的思想。</p>
<p>impute -&gt; analysis -&gt; pool三步走，pool步骤依据Rubin’s Rule.</p>
<p>Bayesian MI:</p>
<ol>
<li><p>根据observed data去得到参数的后验分布，从参数的后验分布得到一个$\theta$。Missing value从 $f(\theta)$</p>
<p>中获得。然后得到一个 complete data set, 去估计参数。</p>
</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># R mice::md.pattern</span></span><br><span class="line"><span class="comment"># Python</span></span><br><span class="line"><span class="keyword">import</span> missingno <span class="comment">#处理缺失数据 没用过</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> folium <span class="comment">#处理和地图有关的数据时候用的一个API</span></span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h3 id="2-Extreme-Values"><a href="#2-Extreme-Values" class="headerlink" title="2. Extreme Values"></a>2. Extreme Values</h3><h2 id="3-特征规范化"><a href="#3-特征规范化" class="headerlink" title="3. 特征规范化"></a>3. 特征规范化</h2><h3 id="1-连续变量"><a href="#1-连续变量" class="headerlink" title="1. 连续变量"></a>1. 连续变量</h3><ol>
<li>Kurtosis峰度，总体所有取值分布于正态分布的相同程度。0表示一致，大于0则陡峭，小于0则平坦。绝对值越大，则与正态分布差异越大。</li>
<li>Skewness，总体分布的对称性。大于0右偏，即长尾拖在右边，小于0左偏</li>
</ol>
<h3 id="2-离散变量"><a href="#2-离散变量" class="headerlink" title="2. 离散变量"></a>2. 离散变量</h3><ol>
<li><p>有序的可以使用1,2,3等。比如疾病的严重程度：轻微/中度/全村吃饭。</p>
</li>
<li><p>无序的dummy variable是一种。one-hot coding是另一种。但是他们比较类似。</p>
</li>
</ol>
<h2 id="4-特征提取和衍生"><a href="#4-特征提取和衍生" class="headerlink" title="4. 特征提取和衍生"></a>4. 特征提取和衍生</h2><ol>
<li>PCA进行降维。协方差矩阵，特征值降序排列。$\frac{\lambda_{1}+…+\lambda_{m}}{\sum_{i=1}^{n}\lambda_{i}}$所占的proportation够大，后面$n-p$个feature可以直接删除。</li>
<li>中间可以进行<strong>特征相关性</strong>分析，也可以降维。</li>
<li>还有一种降维的方式，就是在训练模型的时候引入<strong>惩罚项</strong>, 例如Lasso中的L1, Ridge里面的L2, 前者直接将变量的coef取0值，后者使coef趋于0. </li>
<li>数值变量可以进行互相的<strong>多项式组合</strong>来构造新的变量。</li>
<li>对于分类变量，可以直接将该分类变量的n个取值one-code, 也可以将n个取值划分为sub-group, 然后进行one-code.</li>
<li>当NLP时，TFIDF, Length可能是需要额外考虑的。此处不进行展开讨论，NLP不是研究的范围。</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pivottablejs <span class="comment">#查看数据相关性 没用过</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> pandas_profiling <span class="comment">#数据质量探索的一个包</span></span><br><span class="line"><span class="comment"># rejected表示和其他变量相关性非常高，可以剔除该变量</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h2 id="5-Visualization-可视化"><a href="#5-Visualization-可视化" class="headerlink" title="5. Visualization 可视化"></a>5. Visualization 可视化</h2><p>上述所有步骤，都可以结合Visualization。单纯的查询数值，没有可视化后那么直观。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># seaborn</span></span><br><span class="line"><span class="comment"># matplotlib</span></span><br><span class="line"></span><br><span class="line">sns.pairplot() <span class="comment">#连续变量之间</span></span><br><span class="line">scatterplot() <span class="comment">#离散变量之间</span></span><br><span class="line">boxplot/violinplot() <span class="comment">#连续X离散</span></span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h2 id="6-建模"><a href="#6-建模" class="headerlink" title="6. 建模"></a>6. 建模</h2><p>参照另外一篇<strong>ML_DS</strong>.</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2020/11/19/My-First-Talk/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Yulin Wang">
      <meta itemprop="description" content="要讲武德，要努力学习。">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="浑元形意太极门大弟子">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2020/11/19/My-First-Talk/" class="post-title-link" itemprop="url">My first blog</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2020-11-19 15:24:07" itemprop="dateCreated datePublished" datetime="2020-11-19T15:24:07+08:00">2020-11-19</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2020-11-25 14:22:18" itemprop="dateModified" datetime="2020-11-25T14:22:18+08:00">2020-11-25</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="如何撰写一篇Markdown"><a href="#如何撰写一篇Markdown" class="headerlink" title="如何撰写一篇Markdown"></a>如何撰写一篇Markdown</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">$ hexo clean</span><br><span class="line"># Clean files.</span><br><span class="line"></span><br><span class="line">$ hexo new &quot;The title of new files&quot;</span><br><span class="line"></span><br><span class="line">$ hexo generate</span><br><span class="line"># 生成</span><br><span class="line"></span><br><span class="line">$ hexo server</span><br><span class="line"># Localcost:4000 本地查看</span><br><span class="line"></span><br><span class="line">$ hexo deploy</span><br><span class="line"># 远程部署至Github</span><br></pre></td></tr></table></figure>
<ol>
<li>使用hexo n “Name of Your New Document”</li>
<li>使用typora进行Markdown编辑，反正比Word好用。</li>
</ol>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2020/11/18/The-Introduction-of-Myself/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Yulin Wang">
      <meta itemprop="description" content="要讲武德，要努力学习。">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="浑元形意太极门大弟子">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2020/11/18/The-Introduction-of-Myself/" class="post-title-link" itemprop="url">Welcome to my personal CV github.</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2020-11-18 18:42:42" itemprop="dateCreated datePublished" datetime="2020-11-18T18:42:42+08:00">2020-11-18</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2020-11-25 14:20:11" itemprop="dateModified" datetime="2020-11-25T14:20:11+08:00">2020-11-25</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>我拥有的技能：还是数学有意思，但是上升到测度论就打扰了。</p>
<p>我喜欢的课程：Statistics and Machine Learning.</p>
<h2 id="掌握的杂七杂八的技能以及一些课程。"><a href="#掌握的杂七杂八的技能以及一些课程。" class="headerlink" title="掌握的杂七杂八的技能以及一些课程。"></a>掌握的杂七杂八的技能以及一些课程。</h2><h3 id="Bayesian-Theory-and-Some-Algorithm"><a href="#Bayesian-Theory-and-Some-Algorithm" class="headerlink" title="Bayesian Theory and Some Algorithm."></a>Bayesian Theory and Some Algorithm.</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">1. Metropolis Hastings and Gibbs Sampling</span><br><span class="line">2. Simulated Annealing</span><br><span class="line">3. Climbing Hills</span><br><span class="line">4. Hidden Markov Model</span><br><span class="line">5. For Regression Trees:</span><br><span class="line">        1) ID 3</span><br><span class="line">        2) C4.5</span><br><span class="line">        3) CART</span><br><span class="line">6. Linear Regression and Nonlinear(especially <span class="keyword">for</span> the Basis Function Expansions),</span><br><span class="line">some common models are like RBF(Radial) and Kernel Tricks.</span><br><span class="line">7. SVM(Support Vector Machines). Project low dimensions to high dimension.</span><br></pre></td></tr></table></figure>
<h3 id="Codes-ability"><a href="#Codes-ability" class="headerlink" title="Codes ability"></a>Codes ability</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">1. Python(Pycharm, Jupyter and Spyder) Anaconda. </span><br><span class="line">        1) NLP: nltk </span><br><span class="line">        2) ML: scikit-learn</span><br><span class="line">        3) Computer Vision: OpenCV</span><br><span class="line">2. R Language: visualization and Rmarkdown(I really love it.)</span><br><span class="line">3. C(Codeblocks)</span><br></pre></td></tr></table></figure>
<h3 id="Something-useful"><a href="#Something-useful" class="headerlink" title="Something useful?"></a>Something useful?</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">1. Github and Git</span><br><span class="line">2. Typora <span class="keyword">for</span> Markdown</span><br><span class="line">3. Link Github and hexo (node.js)</span><br></pre></td></tr></table></figure>
<h3 id="Language-ability"><a href="#Language-ability" class="headerlink" title="Language ability"></a>Language ability</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">English: IELTS 7.0; GRE 320; CET 6 567</span><br><span class="line">Chinese: weibo 冲浪高手</span><br><span class="line">Janpanes: ...</span><br></pre></td></tr></table></figure>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  


  



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">Yulin Wang</p>
  <div class="site-description" itemprop="description">要讲武德，要努力学习。</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">6</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
  </nav>
</div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2020</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Yulin Wang</span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Gemini</a> 强力驱动
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>




  















  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  

</body>
</html>
